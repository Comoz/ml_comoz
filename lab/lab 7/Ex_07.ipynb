{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9457fdc4-1690-4c41-8e88-1597fc2a687c",
      "metadata": {
        "id": "9457fdc4-1690-4c41-8e88-1597fc2a687c"
      },
      "source": [
        " # MPA-MLF, Lab 7 - Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa6be62-ed89-4801-b297-f1a1211ce297",
      "metadata": {
        "id": "ffa6be62-ed89-4801-b297-f1a1211ce297"
      },
      "source": [
        "## Exercise - Hand-written digits recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89808395-7ffe-4df1-91e5-f31ce090b932",
      "metadata": {
        "id": "89808395-7ffe-4df1-91e5-f31ce090b932"
      },
      "source": [
        "Create CNN which will process and recognize handwritten digits. For this purposes please use the MNIST database (Modified National Institute of Standards and Technology database) which is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n",
        "The datasample of the MNIST datasets can be see in the following picture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b6e2b8-73da-4476-b47d-2f4aafead049",
      "metadata": {
        "id": "87b6e2b8-73da-4476-b47d-2f4aafead049"
      },
      "source": [
        "![mnist_data_sample.png](attachment:eb3e0d6a-ccb0-499d-9847-ecbc554dbce0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c65f181-d971-4f0e-ba63-17c242a65d6c",
      "metadata": {
        "id": "0c65f181-d971-4f0e-ba63-17c242a65d6c"
      },
      "source": [
        "### Task description"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6b04cf-eeec-404a-824b-f9aa1d3b7d7a",
      "metadata": {
        "id": "1b6b04cf-eeec-404a-824b-f9aa1d3b7d7a"
      },
      "source": [
        "In the terms of machine learning, the Hand-written digits recognition can be threated as a multi-class classification problem. This is very important knowledge to structure our model in the correct way (Especially the output-layer, including the number of neurons and activations function and the overall loss function and classification metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b724de14-3931-4983-b443-7e0106d190dc",
      "metadata": {
        "id": "b724de14-3931-4983-b443-7e0106d190dc"
      },
      "source": [
        "### 0. Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffd9b11-f9c1-4b3f-8dd1-cbb18487a075",
      "metadata": {
        "id": "1ffd9b11-f9c1-4b3f-8dd1-cbb18487a075"
      },
      "source": [
        "Import the all necessary libraries, you can get inspired by the previous exercises. You can improst the libraries gradually, when do you progressing with the task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1f692dd4-0262-4e7a-b029-69d8280f14d2",
      "metadata": {
        "id": "1f692dd4-0262-4e7a-b029-69d8280f14d2"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "###################################\n",
        "# Write your own code here #\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.optimizers import Adamax\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "\n",
        "###################################\n",
        "font = {'weight' : 'bold',\n",
        "        'size'   : 12}\n",
        "\n",
        "matplotlib.rc('font', **font)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4af957-fee1-4806-9d68-797d74c332df",
      "metadata": {
        "id": "9c4af957-fee1-4806-9d68-797d74c332df"
      },
      "source": [
        "### 1. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33cf2443-c2ed-4aaa-9b10-fa598a4fb6cb",
      "metadata": {
        "id": "33cf2443-c2ed-4aaa-9b10-fa598a4fb6cb"
      },
      "source": [
        "#### 1.1 Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd860d81-ab4d-48d2-a071-e0e8aec8000f",
      "metadata": {
        "id": "dd860d81-ab4d-48d2-a071-e0e8aec8000f"
      },
      "source": [
        "You can load the dataset using the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1b6f52dc-788b-4481-95f2-c4de31cae037",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b6f52dc-788b-4481-95f2-c4de31cae037",
        "outputId": "d892ec9b-dbae-48ac-fbc6-b1e3c6f57a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853727ee-5836-4345-84dd-b0135b33e6d3",
      "metadata": {
        "id": "853727ee-5836-4345-84dd-b0135b33e6d3"
      },
      "source": [
        "#### 1.2 Dataset examination"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93e9f48e-3192-494a-9b0f-e2f66a7c286e",
      "metadata": {
        "id": "93e9f48e-3192-494a-9b0f-e2f66a7c286e"
      },
      "source": [
        "Using the following code, display random images,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "522d7c5e-f50b-46d0-b79d-799d40ff2f1e",
      "metadata": {
        "id": "522d7c5e-f50b-46d0-b79d-799d40ff2f1e"
      },
      "outputs": [],
      "source": [
        "def display_random_images(x_data: np.array, y_data: np.array, count: int = 10) -> None:\n",
        "  index = np.array(len(x_data))\n",
        "  selected_ind = np.random.choice(index, count)\n",
        "\n",
        "  selected_img = x_data[selected_ind]\n",
        "  selected_labels = y_data[selected_ind]\n",
        "  concat_img = np.concatenate(selected_img, axis=1)\n",
        "\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.imshow(concat_img, cmap=\"gray\")\n",
        "\n",
        "  for id_label, label in enumerate(selected_labels):\n",
        "    plt.text(14 + 28*id_label, 28*(5/4), label)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "452ea9c5-8438-4b10-8a0e-ef0e418ba5a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "452ea9c5-8438-4b10-8a0e-ef0e418ba5a0",
        "outputId": "bee0980a-8c9d-4003-d18e-5bc7da11cb2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display_random_images(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0e3651-3921-4485-bdae-3731afcf03a2",
      "metadata": {
        "id": "6c0e3651-3921-4485-bdae-3731afcf03a2"
      },
      "source": [
        "Examine the dataset. Answer for yourself the following questions:\n",
        "\n",
        "- What kind of data occurs in our dataset?\n",
        "- How many data samples do we have in train and test datasets?\n",
        "- How many colour channels does the input variable have?\n",
        "- What is the size of the input images?\n",
        "- What is the necessary preprocessing of the input data X?\n",
        "- How many classes do we have in target variable?\n",
        "- What is the necessary preprocessing of target variable y?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08273dd4-05d0-4cd8-b989-eca8a4d1328a",
      "metadata": {
        "id": "08273dd4-05d0-4cd8-b989-eca8a4d1328a"
      },
      "source": [
        "#### 1.3 Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c16ee55-312f-4ee5-86cd-e09426e16e82",
      "metadata": {
        "id": "2c16ee55-312f-4ee5-86cd-e09426e16e82"
      },
      "source": [
        "Perform the necessary data preprocessing. The best way to preprocess the data would be one hot encoding for the target variable and normalization for the input variable (using min-max or z-score normalization)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CBgfu5XBUKt",
        "outputId": "a780c7fb-c8ac-49dd-dbed-b4256de662f0"
      },
      "id": "0CBgfu5XBUKt",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e07697fb-0feb-4df1-8ed2-b7b20f0c015f",
      "metadata": {
        "id": "e07697fb-0feb-4df1-8ed2-b7b20f0c015f"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# Write your own code here #\n",
        "#X is already a 2D image, not needing any preprocessing yet (it will have to be flatten before the fully connected layer)\n",
        "#Y will need to be one-hot-encode\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "\n",
        "y_train_encoded = to_categorical(y_train, num_classes=10)\n",
        "\n",
        "\n",
        "\n",
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d20d5fef-cbca-48a2-844f-c9638f0b6bf9",
      "metadata": {
        "id": "d20d5fef-cbca-48a2-844f-c9638f0b6bf9"
      },
      "source": [
        "### 2. Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea554fcf-7acd-4453-b18d-b4982f6a10eb",
      "metadata": {
        "id": "ea554fcf-7acd-4453-b18d-b4982f6a10eb"
      },
      "source": [
        "In this section, your task will be to define the model architecture. The intial structure can be defined as follows:\n",
        "\n",
        "Input_layer -> Convolutional_layer(kernel_size=(3,3), no_channels=32) -> Maxpooling_layer(kernel_size=(2, 2)) -> Flatten_layer -> Dense_layer (num_classes)\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffd3896-079e-4758-9579-387f33af9691",
      "metadata": {
        "id": "0ffd3896-079e-4758-9579-387f33af9691"
      },
      "source": [
        "#### 2.1 Define the model structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "77238bdf-ad74-4246-920d-a1dc28564306",
      "metadata": {
        "id": "77238bdf-ad74-4246-920d-a1dc28564306"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "###################################\n",
        "# Write your own code here #\n",
        "model.add(Conv2D(64,5,activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128,3,activation='sigmoid',input_shape=(28,28,1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "###################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fde4b3eb-90e1-4724-89df-0db1872560d4",
      "metadata": {
        "id": "fde4b3eb-90e1-4724-89df-0db1872560d4"
      },
      "source": [
        "#### 2.2 Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a467f8fb-8bfc-4cd4-9eee-820c1b9b5a52",
      "metadata": {
        "id": "a467f8fb-8bfc-4cd4-9eee-820c1b9b5a52"
      },
      "source": [
        "Build the model, use the relevant metrics, optimizer and loss function. While choosing the metrics and loss function, consider fact that we are are trying to solve the multiclass classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e6d1a924-9e2f-4ca2-b4d6-4724f51ae065",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6d1a924-9e2f-4ca2-b4d6-4724f51ae065",
        "outputId": "16608d84-2ee5-442a-b4aa-ac575345b87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 24, 24, 64)        1664      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 12, 12, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3200)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               409728    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 486538 (1.86 MB)\n",
            "Trainable params: 486538 (1.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###################################\n",
        "# Write your own code here #\n",
        "\n",
        "optimizer = Adamax(learning_rate = 0.0001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "###################################\n",
        "#For some reason I can not launch it at that point without a crash\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f146b70f-2e8c-484f-abfd-6fc4a8b8177b",
      "metadata": {
        "id": "f146b70f-2e8c-484f-abfd-6fc4a8b8177b"
      },
      "source": [
        "### 3. Training stage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1de787-9e40-47e2-bc54-44ccd1864357",
      "metadata": {
        "id": "cb1de787-9e40-47e2-bc54-44ccd1864357"
      },
      "source": [
        "#### 3.1 Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9c7722-aed7-4b2d-a292-572921f0734b",
      "metadata": {
        "id": "2b9c7722-aed7-4b2d-a292-572921f0734b"
      },
      "source": [
        "train your model, define the relevant hyperparameters (no. epochs, batch_size), use 20p of the training data for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4f858566-601d-4873-ad02-a0635bd8f526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f858566-601d-4873-ad02-a0635bd8f526",
        "outputId": "11935425-96b1-4bce-e4a6-218850c57289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 0.0393 - val_accuracy: 0.9876\n",
            "Epoch 2/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0131 - accuracy: 0.9977 - val_loss: 0.0388 - val_accuracy: 0.9883\n",
            "Epoch 3/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0129 - accuracy: 0.9978 - val_loss: 0.0386 - val_accuracy: 0.9885\n",
            "Epoch 4/40\n",
            "320/320 [==============================] - 2s 8ms/step - loss: 0.0128 - accuracy: 0.9979 - val_loss: 0.0387 - val_accuracy: 0.9887\n",
            "Epoch 5/40\n",
            "320/320 [==============================] - 2s 8ms/step - loss: 0.0126 - accuracy: 0.9980 - val_loss: 0.0385 - val_accuracy: 0.9883\n",
            "Epoch 6/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.0387 - val_accuracy: 0.9883\n",
            "Epoch 7/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0122 - accuracy: 0.9980 - val_loss: 0.0386 - val_accuracy: 0.9878\n",
            "Epoch 8/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0122 - accuracy: 0.9979 - val_loss: 0.0386 - val_accuracy: 0.9883\n",
            "Epoch 9/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.0390 - val_accuracy: 0.9883\n",
            "Epoch 10/40\n",
            "320/320 [==============================] - 3s 8ms/step - loss: 0.0119 - accuracy: 0.9981 - val_loss: 0.0383 - val_accuracy: 0.9884\n",
            "Epoch 11/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0118 - accuracy: 0.9981 - val_loss: 0.0382 - val_accuracy: 0.9884\n",
            "Epoch 12/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0116 - accuracy: 0.9981 - val_loss: 0.0382 - val_accuracy: 0.9884\n",
            "Epoch 13/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0114 - accuracy: 0.9982 - val_loss: 0.0385 - val_accuracy: 0.9881\n",
            "Epoch 14/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.0381 - val_accuracy: 0.9883\n",
            "Epoch 15/40\n",
            "320/320 [==============================] - 3s 8ms/step - loss: 0.0111 - accuracy: 0.9983 - val_loss: 0.0383 - val_accuracy: 0.9886\n",
            "Epoch 16/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.0381 - val_accuracy: 0.9884\n",
            "Epoch 17/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0109 - accuracy: 0.9983 - val_loss: 0.0388 - val_accuracy: 0.9889\n",
            "Epoch 18/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.0386 - val_accuracy: 0.9888\n",
            "Epoch 19/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9886\n",
            "Epoch 20/40\n",
            "320/320 [==============================] - 3s 8ms/step - loss: 0.0105 - accuracy: 0.9984 - val_loss: 0.0381 - val_accuracy: 0.9886\n",
            "Epoch 21/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0103 - accuracy: 0.9984 - val_loss: 0.0378 - val_accuracy: 0.9885\n",
            "Epoch 22/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0380 - val_accuracy: 0.9885\n",
            "Epoch 23/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9886\n",
            "Epoch 24/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9887\n",
            "Epoch 25/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.0376 - val_accuracy: 0.9883\n",
            "Epoch 26/40\n",
            "320/320 [==============================] - 3s 8ms/step - loss: 0.0097 - accuracy: 0.9986 - val_loss: 0.0379 - val_accuracy: 0.9886\n",
            "Epoch 27/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0096 - accuracy: 0.9987 - val_loss: 0.0381 - val_accuracy: 0.9887\n",
            "Epoch 28/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9887\n",
            "Epoch 29/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0094 - accuracy: 0.9987 - val_loss: 0.0381 - val_accuracy: 0.9887\n",
            "Epoch 30/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0093 - accuracy: 0.9987 - val_loss: 0.0380 - val_accuracy: 0.9887\n",
            "Epoch 31/40\n",
            "320/320 [==============================] - 3s 8ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 0.0376 - val_accuracy: 0.9883\n",
            "Epoch 32/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0091 - accuracy: 0.9988 - val_loss: 0.0374 - val_accuracy: 0.9886\n",
            "Epoch 33/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.0376 - val_accuracy: 0.9890\n",
            "Epoch 34/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0088 - accuracy: 0.9989 - val_loss: 0.0374 - val_accuracy: 0.9890\n",
            "Epoch 35/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.0374 - val_accuracy: 0.9890\n",
            "Epoch 36/40\n",
            "320/320 [==============================] - 3s 9ms/step - loss: 0.0086 - accuracy: 0.9989 - val_loss: 0.0379 - val_accuracy: 0.9889\n",
            "Epoch 37/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0085 - accuracy: 0.9989 - val_loss: 0.0376 - val_accuracy: 0.9883\n",
            "Epoch 38/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0084 - accuracy: 0.9989 - val_loss: 0.0375 - val_accuracy: 0.9887\n",
            "Epoch 39/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0082 - accuracy: 0.9991 - val_loss: 0.0377 - val_accuracy: 0.9883\n",
            "Epoch 40/40\n",
            "320/320 [==============================] - 2s 7ms/step - loss: 0.0082 - accuracy: 0.9990 - val_loss: 0.0379 - val_accuracy: 0.9885\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "# Write your own code here #\n",
        "callback = EarlyStopping(monitor='loss', patience=3)\n",
        "history = model.fit(X_train, y_train_encoded, epochs=40, batch_size=150, validation_split = 0.2, callbacks=[callback])\n",
        "\n",
        "\n",
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45651f2d-5cc4-4896-8edc-f58b50fed605",
      "metadata": {
        "id": "45651f2d-5cc4-4896-8edc-f58b50fed605"
      },
      "source": [
        "#### 3.1 Model Evaluation on validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb4ea2f2-fcc8-4308-82f6-3dbd5857e989",
      "metadata": {
        "id": "eb4ea2f2-fcc8-4308-82f6-3dbd5857e989"
      },
      "source": [
        "Plot the development of the training and validation loss, and training and validation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QosoRwB5Tbdx"
      },
      "outputs": [],
      "source": [
        "#Do not rerun it\n",
        "#X_test = X_test.astype('float32') / 255.0\n",
        "y_test_encoded = to_categorical(y_test, num_classes=10)"
      ],
      "id": "QosoRwB5Tbdx"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "841d1e30-e448-4b53-b3fc-9b97863391bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "841d1e30-e448-4b53-b3fc-9b97863391bb",
        "outputId": "cc98e5c1-0bab-4297-f6be-efa1587aea89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.026842480525374413\n",
            "Test accuracy: 99.11999702453613 %\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "# Write your own code here #\n",
        "\n",
        "score = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print(f'Test accuracy: {score[1]*100} %')\n",
        "\n",
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22cd86fb-6b4c-4299-a077-fec0ab62464c",
      "metadata": {
        "id": "22cd86fb-6b4c-4299-a077-fec0ab62464c"
      },
      "source": [
        "### 4. Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d93b7ad-3416-451d-8762-968f4cf1dd13",
      "metadata": {
        "id": "9d93b7ad-3416-451d-8762-968f4cf1dd13"
      },
      "source": [
        "Evaluate the model on the testing dataset using the relevant metrics. Use the confusion metrics as the one of the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8134b757-aca1-4a0d-a0d2-3a3d0daa8d38",
      "metadata": {
        "id": "8134b757-aca1-4a0d-a0d2-3a3d0daa8d38"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# Write your own code here #\n",
        "\n",
        "\n",
        "\n",
        "###################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a3f72d-1d76-4d98-9f03-1f8293ed6ad6",
      "metadata": {
        "id": "44a3f72d-1d76-4d98-9f03-1f8293ed6ad6"
      },
      "source": [
        "### 5. Hyperparameter tunning and regularization techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6140057-ce25-4e97-ae7b-81a47a30bebc",
      "metadata": {
        "id": "e6140057-ce25-4e97-ae7b-81a47a30bebc"
      },
      "source": [
        "When your code is ready and fully functional, try several changes in the hyperparameters and see how they influence the testing metrics. Try changes in the network structure. You can also try adding regularization techniques such as L1, L2, and Dropout. Based on the development of training and validation loss, try to identify overfitting and avoid it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f4b10b-7487-45f4-8702-267715e4041c",
      "metadata": {
        "id": "f1f4b10b-7487-45f4-8702-267715e4041c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}